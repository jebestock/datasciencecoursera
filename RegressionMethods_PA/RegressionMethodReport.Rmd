---
title: 'Regression Methods: Course Project'
output:
  pdf_document: default
  html_document:
    keep_md: yes
---
Author: Jens Berkmann
<!-- setwd("C:/Users/jb/gitrepos/RegressionMethods/") -->

## Abstract
In this report an investigations on the *mtcars* dataset provided by the R package was done. In particular the relationship between MilesPerGallon and automatic or manual transmission was investigated. It was found that  manual transmission is significantly advantageous over automatic transmission with a difference of 7.24 MilesPerGallon. However, also the number of cylinders, the horse power as well as the weight of the car have an effect on the MilesPerGallon. In particular, when keeping the latter variables constant, cars with manual transmission exhibit on average an increase of 1.81 MilesPerGallon compared to automatic transmission. 






## Exploratory Data Analysis
We first load the data, convert some variables to factor variables and print a summary of the data set. 

```{r}
library(knitr)
data(mtcars)
mtc      <- mtcars
names(mtc)
mtc$cyl  <- as.factor(mtc$cyl)
mtc$vs   <- as.factor(mtc$vs)
mtc$am   <- as.factor(mtc$am)
mtc$gear <- as.factor(mtc$gear)
mtc$carb <- as.factor(mtc$carb)
str(mtc)
```

The dataframe contains of 32 rows (samples, observations) and 11 columns (variables). According to R's help function the meaning of the variables is as follows:

mpg 	 Miles/(US) gallon; 
cyl 	 Number of cylinders; 
disp 	 Displacement (cu.in.); 
hp 	 Gross horsepower; 
drat 	 Rear axle ratio; 
wt 	 Weight (lb/1000); 
qsec 	 1/4 mile time, 
vs 	 V/S; 
am 	 Transmission (0 = automatic, 1 = manual); 
gear 	 Number of forward gears; 
arb 	 Number of carburetors.
 
The variable *mpg* contains the outcome (Miles/(US)gallon) we are interested in. We are also in particular interested in the column *am* which distinguishes between automatic (am=0) and manual transmission (am=1).   As a basic analysis let us plot the outcome *mpg* versus the variable *am*.


```{r}
boxplot(mpg ~ am, data = mtc, xlab = "am (0 = auto, 1 = manual)")
```

It is seen that manual transmission results in higher (better) values of mpg. Moreover, from the plot the separation of the two am-groups are fairly well separated. Still, let's check via t-testing whether the difference in the means is significant.

```{r}
mytest <- t.test(mpg ~ am, data = mtc, var.equal=FALSE)
mytest
```
From the reported fairly small p-value we conclude that the difference in means of $24.39-17.15=$`r 24.39-17.15` is not observed just by accident.

## Regression Models
Let us now turn to the task of (regression) model selection. Via R's regsubsets function we find the best model starting from the model using all predictor variables.

```{r}
library(leaps)
simple <- lm(mpg ~ am, data = mtc)
full   <- lm(mpg ~ .,  data = mtc)
best   <- step(full, data=mtc, direction="both", trace=0) 
summary(best)
```
The best linear model is found to explain 84% of the data variance and uses apart from the variable *am* also the variables *cyl*, *hp* and *wt* which is also meaningful from a common sense perspective.
Let us compare now the simple model (mpg~am) versus the model found by the last optimization method and check its significance. We do this by calling the anova-funcion of R.

```{r}
anova(simple,best)
```
From the reported tiny P-value we conclude that the optimized model *best* explains the data better than the simple model not just by accident. However, when looking at the high P-value of the $beta$-coefficient in the just found model 
```{r}

summary(lm(mpg ~ am+cyl+hp+wt, data = mtc))
```
we have doubts that including *am* is significant.
Therefore we perform a  last verification step by  checking whether excluding the variable *am* from the found model
is significant or not.
```{r}
testfit <- lm(mpg ~ cyl+hp+wt, data = mtc)
anova(best,testfit)
```
From the observed P-value we reject the exclusion of the prediction variable *am*. 
The final model is therefore selected to be 
$mpg = 33.71 -3.03\cdot cyl_6 -2.16\cdot cyl_8 -0.03\cdot hp -2.5\cdot wt + 1.81 am_{manual}$.


##Residual Analysis

We finally do a residual analysis on the selected model *best*.
```{r}
layout(matrix(c(1,2,3,4),2,2))
plot(best)
```

No suspicious patterns are found in the upper left figure. Moreover, from the lower left plot the normality assumption of the data seems to be sufficiently given.


